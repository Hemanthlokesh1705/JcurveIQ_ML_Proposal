{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Financial Document QA & Risk Detection Pipeline\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xgAsoFp9Uu01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Architecture & Models\n",
        "\n",
        "## Baseline Design (MVP, Reproducible)\n",
        "- **Chunking:** Simple overlapping splitter (300–500 tokens).  \n",
        "\n",
        "### Retriever\n",
        "- **Sparse:** BM25 via Elasticsearch (keyword precision for numeric/legal phrases).  \n",
        "- **Dense:** sentence-transformers/all-MiniLM-L6-v2 → FAISS flat index for quick prototyping.  \n",
        "- **Fusion:** Union top results from BM25 and dense retriever (deduplicate).  \n",
        "\n",
        "### Generator\n",
        "- **Model:** API LLM (GPT-4/GPT-4o)  \n",
        "- **Prompt:** Strict prompt includes only retrieved chunks; must cite chunk IDs and source doc metadata.  \n",
        "- **Temperature:** 0.0 (deterministic output).  \n",
        "\n",
        "### Audit Trail\n",
        "- Save retrieved chunk IDs, prompt text, model response, tokens used.  \n",
        "\n",
        "### Fishiness Detector (Baseline)\n",
        "- Rule-based heuristics:\n",
        "  - YoY ratio thresholds  \n",
        "  - Large goodwill changes  \n",
        "  - Related-party keywords  \n",
        "\n",
        "### Why Baseline?\n",
        "- Fast to implement, explainable, cheap.  \n",
        "- BM25 catches numeric/keyword queries that embeddings might miss.  \n",
        "- MiniLM provides basic semantic coverage.  \n",
        "- Good for initial human reviewers.  \n",
        "\n",
        "### Scalability\n",
        "- Elasticsearch clusters scale horizontally.  \n",
        "- FAISS with IVF/PQ can scale to tens of millions of vectors.  \n",
        "- LLM via API scales by concurrency quotas.  \n",
        "\n",
        "---\n",
        "\n",
        "# Candidate Models (In Depth)\n",
        "\n",
        "## A. Retrieval Improvements\n",
        "### 1. Domain-specific Dense Embeddings\n",
        "- **Model:** FinBERT / FinDomain sentence-transformer / OpenAI financial-tuned embeddings.  \n",
        "- **Why:** Captures domain semantics (e.g., “reserve” vs “provision”), increases retrieval precision.  \n",
        "- **Scale:** Same vector infrastructure; slightly heavier encoder amortized at ingest time.  \n",
        "\n",
        "### 2. Hybrid Retrieval (BM25 + Dense + Keyword Expansions)\n",
        "- **What:** Query both systems and merge results. Add query expansion with company-specific synonyms and accounting terms.  \n",
        "- **Why:** Maximizes recall — BM25 catches exact matches, dense catches paraphrases.  \n",
        "- **Scale:** Two systems operate, fusion is O(k). Cache common queries.  \n",
        "\n",
        "### 3. Cross-Encoder Reranker\n",
        "- **Model:** Fine-tuned cross-encoder (BERT / FinBERT).  \n",
        "- **What:** Re-ranks top N candidates (e.g., 100) from the retriever.  \n",
        "- **Why:** Improves precision@k, reduces hallucination risk.  \n",
        "- **Scale:** Run on GPUs, only on top N. Can distill later for latency.  \n",
        "\n",
        "---\n",
        "\n",
        "## B. Evidence Extraction & Citation\n",
        "### Span-Extractor / Token-Level QA\n",
        "- **Model:** Fine-tuned transformer for extractive QA (e.g., RoBERTa/FinBERT QA).  \n",
        "- **Why:** Extracts exact supporting sentences/phrases → improves citation fidelity, reduces LLM context size.  \n",
        "- **Scale:** Run per candidate chunk; batch inference. Fine-tune on labeled (question → supporting span) pairs.  \n",
        "\n",
        "---\n",
        "\n",
        "## C. Generation Improvements\n",
        "### Instruction-Tuned / LoRA-Fine-Tuned Generator\n",
        "- **What:** Fine-tune smaller LLM (7B–13B) on financial QA + citation templates.  \n",
        "- **Why:** Reduces cost, improves adherence to citation rules and tone.  \n",
        "- **Scale:** Serve on GPU cluster; LoRA keeps resources manageable.  \n",
        "\n",
        "### Citation-Aware Decoding Constraints\n",
        "- **What:** Enforce output patterns (“Claim — [Source: DOC_ID|CHUNK_ID]”) and post-validate with span extractor.  \n",
        "- **Why:** Required for audit; reduces hallucination.  \n",
        "\n",
        "---\n",
        "\n",
        "## D. Verification & Safety\n",
        "### Verifier / Entailment Classifier\n",
        "- **What:** NLI model checks if each claim is supported by cited evidence (entail / contradict / unknown).  \n",
        "- **Why:** Automates early detection of hallucinations; triggers human review if low confidence.  \n",
        "- **Scale:** Fast, small transformer per claim.  \n",
        "\n",
        "---\n",
        "\n",
        "## E. Fishiness / Accounting Risk Detector (Hybrid)\n",
        "- **Signals:**\n",
        "  - Rule flags: “related party”, “restatement”, “one-time gain”, ambiguous language.  \n",
        "  - Supervised classifier: Paragraph embeddings → suspicious vs normal; trained on analyst-labeled data.  \n",
        "  - Time-series anomalies: z-score / isolation forests on ratios (ROA, gross margin, receivables/sales), peer-relative deviations.  \n",
        "- **Why:** Combines linguistic + numeric signals to reduce false positives; human-in-the-loop improves precision.  \n",
        "- **Scale:** Run offline at ingestion and on-demand.  \n",
        "\n",
        "---\n",
        "\n",
        "## F. Operational Acceleration\n",
        "### Distillation / Approximate Reranker\n",
        "- **What:** Distill cross-encoder into faster bi-encoder or train lightweight reranker for near cross-encoder accuracy.  \n",
        "- **Why:** Reduces latency while keeping precision.  \n"
      ],
      "metadata": {
        "id": "tnDO-8dvTAKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF sentence-transformers faiss-cpu openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxEamM12RTqB",
        "outputId": "c0411733-0080-4db6-a473-a44688abc8ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF, faiss-cpu\n",
            "Successfully installed PyMuPDF-1.26.5 faiss-cpu-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------------\n",
        "# 1. Imports\n",
        "# -------------------------------\n",
        "import fitz  # PyMuPDF for PDF extraction\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "import re\n",
        "import openai\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mukcByr3rsv7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 2. OpenAI API Key setup\n",
        "# -------------------------------\n",
        "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n"
      ],
      "metadata": {
        "id": "n8zPr9hKR2ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 3. Sample PDF/Text ingestion\n",
        "# -------------------------------\n",
        "sample_docs = {\n",
        "    \"10K_2024.pdf\": \"\"\"\n",
        "    Company ABC reports a 20% increase in revenue YoY, with operating income up 15%.\n",
        "    Risk factors include potential litigation and supply chain disruptions.\n",
        "    \"\"\",\n",
        "    \"Earnings_Call_Q1.txt\": \"\"\"\n",
        "    Management mentions that cash flow is strong, but receivables have increased.\n",
        "    There is a one-time gain from asset sale.\n",
        "    \"\"\"\n",
        "}"
      ],
      "metadata": {
        "id": "wxFAMKEvR7tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 4. Chunking function\n",
        "# -------------------------------\n",
        "def chunk_text(text, chunk_size=50, overlap=10):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = min(start + chunk_size, len(words))\n",
        "        chunk = \" \".join(words[start:end])\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "doc_chunks = []\n",
        "metadata = []\n",
        "for doc_name, text in sample_docs.items():\n",
        "    chunks = chunk_text(text)\n",
        "    for i, c in enumerate(chunks):\n",
        "        doc_chunks.append(c)\n",
        "        metadata.append(f\"{doc_name} | chunk {i+1}\")"
      ],
      "metadata": {
        "id": "zNpGrz0tR_G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 5. Embeddings\n",
        "# -------------------------------\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(doc_chunks, convert_to_numpy=True)"
      ],
      "metadata": {
        "id": "IvB5aPHXSBjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 6. FAISS Index\n",
        "# -------------------------------\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n"
      ],
      "metadata": {
        "id": "NvP_BGBmSD5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 7. Fishiness Detector\n",
        "# -------------------------------\n",
        "def is_fishy(text):\n",
        "    flags = []\n",
        "    if re.search(r'\\b\\d{1,2}% increase\\b', text):\n",
        "        flags.append(\"Revenue/Income spike\")\n",
        "    if re.search(r'one-time gain|extraordinary item', text, re.I):\n",
        "        flags.append(\"One-time gain\")\n",
        "    if re.search(r'litigation|risk factor|uncertain', text, re.I):\n",
        "        flags.append(\"Potential risk\")\n",
        "    return flags\n"
      ],
      "metadata": {
        "id": "0bvWKiMJSHAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 8. Retrieval function\n",
        "# -------------------------------\n",
        "def retrieve(query, k=3):\n",
        "    query_emb = model.encode([query], convert_to_numpy=True)\n",
        "    distances, indices = index.search(query_emb, k)\n",
        "    results = []\n",
        "    for idx in indices[0]:\n",
        "        text = doc_chunks[idx]\n",
        "        results.append({\n",
        "            \"text\": text,\n",
        "            \"source\": metadata[idx],\n",
        "            \"fishy_flags\": is_fishy(text)\n",
        "        })\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "bWuLa66oSTqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 9. GPT Answer Generation\n",
        "# -------------------------------\n",
        "def generate_answer_gpt(query, retrieved_chunks):\n",
        "    context = \"\"\n",
        "    for r in retrieved_chunks:\n",
        "        flags = \", \".join(r['fishy_flags']) if r['fishy_flags'] else \"None\"\n",
        "        context += f\"[Source: {r['source']}, Fishy: {flags}] {r['text']}\\n\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an equity research assistant.\n",
        "    Use the following context from company documents to answer the query below.\n",
        "    Cite the source for each fact using the format [Source: DOC|Chunk].\n",
        "    Highlight anything fishy using ⚠️ if flagged.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Query: {query}\n",
        "\n",
        "    Provide a concise answer with citations.\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n"
      ],
      "metadata": {
        "id": "EgB8x9HxSXsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 10. Example Usage\n",
        "# -------------------------------\n",
        "query = \"Explain revenue growth and risks\"\n",
        "retrieved = retrieve(query)\n",
        "answer = generate_answer_gpt(query, retrieved)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"\\nTop retrieved chunks with citations and fishiness flags:\")\n",
        "for i, r in enumerate(retrieved):\n",
        "    print(f\"{i+1}. [{r['source']}] {r['text']}\")\n",
        "    if r['fishy_flags']:\n",
        "        print(f\"   ⚠️ Fishy Flags: {', '.join(r['fishy_flags'])}\")\n",
        "\n",
        "print(\"\\nGenerated GPT Answer:\\n\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "uAPaeWhoSZzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lHywWMGkS1EH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}